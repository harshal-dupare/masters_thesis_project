\chapter{Datasets}

% \section{Counter-Speech Generation}
% We conducted experiments (mentioned in previous chapter) on a diverse corpus of online speech and try to evaluate the quality of generated responses.
% Our findings after training language-generation models on datasets like REDDIT and GAB(\textbf{\cite{qian2019benchmark}}) indicate that the responses generated by the model to given hate-instances were quite generic. 
% \par 
% The intervention responses can be particularly grouped in the "Denouncing the Hate-Speech Category"(\textbf{\cite{mathew2019thou}}) which lacks diversity and the effectiveness to fight extremism.


\section{Counterspeech datasets} 
In order to evaluate our approach we use three public datasets which contain hate speech and its corresponding counterspeech. The datasets mainly differ in their collection strategy and the source from where the hateful posts were taken. The details of these datasets are noted in Table \ref{tab:cs_dataset}. Reddit and Gab datasets contain $5,257$ and $14,614$ hate speech instances respectively~\cite{qian2019benchmark}. We only take the hate speech instances and discard the other non hateful conversation.  We use the English part of the CONAN dataset~\cite{chung2019conan} which contains $408$ hate speech instances.  The counterspeech in Gab and Reddit datasets were written by AMT workers, whereas for CONAN the counterspeech was written by expert NGO operators.

We further made hate speech and counterspeech pairs from these datasets such that each hate speech was associated with one counterspeech. Finally, we ended up with 3,864, 14,223, 41,580 datapoints for CONAN, Reddit and Gab respectively. We split each dataset randomly into train, validation, test set with 80\% for training, 10\% each for validation and testing.
\begin{table}[!htpb]
\scriptsize
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Dataset} & \textbf{Source-H} & \textbf{Source-C} & \textbf{Hate instances} & \textbf{Total pairs} \\ \hline
CONAN       & synthetic     & expert     &   408     &   3,864      \\ 
Reddit      & reddit        & crowd     &   5,257    &   14,223     \\ 
Gab        & gab           & crowd      &   14,614    &   41,580     \\ \hline
\end{tabular}
\caption{\scriptsize{This table presents the source of hate speech(Source-H), source of counterspeech(Source-C), hate speech instances and the total pairs for each of the CONAN~\cite{chung2019conan}, Reddit and Gab dataset~\cite{qian2019benchmark}.}}
\label{tab:cs_dataset}
\end{table}




\section{Attribute datasets}
We control several attributes in the generated counterspeech. We selected these attributes following the recommended strategies for counterspeech~\cite{benesch2016considerations} and properties of responses in human conversation~\cite{10.1145/3290605.3300705}.

\textbf{\textit{Politeness}}: One of the properties of counterspeech as suggested by \citet{benesch2016considerations} is empathy. As a first step in that direction we tried to make the generated counterspeech more polite. A recent paper~\cite{madaan2020politeness} released a dataset of around 1.39 million posts comprising polite and non-polite sentences. The sentences were automatically labeled using a state-of-the-art model for politeness detection ~\cite{niu2018polite} which labeled the dataset into nine classes (P1-P9). A post having P9 label being the most polite. As recommended by the authors~\cite{madaan2020politeness}, we considered P9 as the polite part and others (P1-P8) as non-polite. 

\newline

\textbf{\textit{Detoxification}}: \citet{benesch2016considerations} also noted several strategies which are discouraged while writing counterspeech. One of these discouraged strategies are hostile or aggressive behaviour. These include silencing, toxic language and harassment of the controversial user. In practice such counterspeech, often cause a backfire effect (i.e., stronger adherence to original speech) and escalation of hateful rhetoric. To detox any hostile counterspeech generated by the generation model, we use the Jigsaw Toxic Comment Classification Challenge Dataset\footnote{https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data}. This dataset contains text samples having `toxic' and `non-toxic' labels. The toxic labels indicate the presence of profanity, obscenity threats, insults or identity hate. We stratified-split the released training dataset randomly into 90\% training and 10\% validation sets. The test set is already released separately with the dataset. 
We trained a \textsc{GeDi} model considering toxic as the positive label and non-toxic as the negative label. While generating using \textsc{GeDi}, we guide the generation toward the negative class (non-toxic). 

\newline

\textbf{\textit{Emotion:}} Another important aspect of conversation is communicating different emotions. Multiple studies~\cite{prendinger2005empathic,partala2004effects} found that systems expressing emotions are more capable of providing user satisfaction. In case of counterspeech, emotions might enhance the effect of the generated counterspeech. For example, `sadness' as an emotion can be added when the counter speakers affiliate themselves with the target group. Similarly, `joy' can used to convey positivity in the counterspeech. Hence such emotional control will help generating diverse and effective counterspeech suggestions for the counter speaker to choose from.

\newline
In order to control the emotion while generating a counterspeech, we used a large dataset~\cite{saravia2018carer} of 416,809 datapoints comprising posts having seven emotions -- `sadness', `joy', `fear', `anger', `surprise', and `love'. For this paper, we did not consider - `love' and `surprise' emotions as these had less than 10\% posts in the dataset. We stratified-split each dataset randomly into training, validation, and test set with 80\% for training, and 10\% for both validation and testing. We consider each emotion as a separate attribute and trained a \textsc{GeDi} model for that emotion by considering it as positive label and other emotions as negative labels. For our experiments, we primarily focus on guiding the models toward the positive class.

A summary statistic of the attribute dataset for each of the task considered is noted in Table \ref{tab:attribute_dataset}.

\begin{table}[!htpb]
\scriptsize
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Dataset}    & \textbf{+ve} & \textbf{-ve} & \textbf{T\textsubscript{r} (\%+ve)} & \textbf{V (\%+ve)} & \textbf{T\textsubscript{e} (\%+ve)} \\ \hline
Polite & p     &   n-p  & 1.12M (20\%) & 137k (20\%) & 137k (20\%)\\ \hline
Toxic   & t        &   n-t   & 143k (10\%) & 16k (10\%) & 153k (4\%)\\ \hline
\multirow{4}{*}{Emotion}    & j           &   o      & 333k (34\%) & 42k (34\%) & 42k (34\%)\\
    & f           &   o      & 333k (11\%) & 42k (11\%) & 42k (11\%)\\ 
    & s           &   o      & 333k (29\%) & 42k (29\%) & 42k (29\%)\\ 
    & a           &   o      & 333k (14\%) & 42k (14\%) & 42k (14\%)\\ \hline
\end{tabular}
\caption{\scriptsize{This table shows the attribute datasets, positive and negative classes and data present in train, validation and test part for each. T\textsubscript{r}: Train, V: Validation, T\textsubscript{e}: Test, p: polite, n-p: non-polite, t: toxic, n-t: non-toxic, s: sadness, j: joy, a: anger, f: fear, o: others. The percentage associated with the  T\textsubscript{r}, V and  T\textsubscript{e} represents the percentage of positive labels.}}
\label{tab:attribute_dataset}
\end{table}


% \section{Crawling an Argument Rich Dataset}

% \par The lack of diverse counter narrations calls for a shift from the hate-counter pairs as the training pairs to argument-response pairs in order for the model to produce quality interventions for a specific hate-instance.

% \par We crawled data from a website called \href{https://www.createdebate.com/}{\textbf{CreateDebate.com}}. CreateDebate is a response to the unreadable jumble of arguments found on traditional internet flame wars and message boards. The CreateDebate platform has a framework that allows users to quickly see which points are the most compelling and which side of a debate is winning.  The strongest arguments rise to the top of the discussion over time, allowing the key points and most important considerations to be distilled.

% \newpage

% People may engage in debates on any subject they care about and the debates can be categorized into the following two categories: 
% \begin{itemize}
%     \item \textbf{For-Against Debate} : A For/Against Debate is for issues that can be easily broken down in to two distinct sides.
%     \item \textbf{Perspectives Debate} : A Perspectives Debate is an open question where the positions are tracked by tags and can't be broken down easily in to two sides.
% \end{itemize}

% \vspace{1.5cm}
% \begin{figure}[h]
%     \centering
%     \includegraphics[scale=2,width=1.0\linewidth]{Graphics/Data-creation (1).jpg}
%     \caption{Pipeline for Dataset-Construction.}
%     \label{fig:DATA-CREATE}
% \end{figure}

% \subsection{\textbf{Crawling Process : }}Both of these debates allows the users to write an argument in support/against the corresponding debate argument in a discussion. We went through the following steps to accumulate this argument rich dataset : 
% \newpage
% \begin{itemize}
%     \item We collected all the discussions having one of the keywords that supposedly helps in crawling discussions with the presence of hate-instances in them.
%     \item  We further used the implications from the Social-Bias Frames Dataset \textbf{\cite{sap-etal-2020-social}} and excluded the topics which had the \textbf{similarity score less than 0.4} with one of the implications which left us with \textbf{526 discussions} in total after the exclusion.
%     \begin{itemize}
%         \item \textbf{SOCIAL BIAS FRAMES} seek to reflect the multiple pragmatic definitions relevant to social bias implications by incorporating categorical and free-text annotations, for example, the sentence "we shouldn't lower our standards to hire more women" implies that "women are less skilled."
%         \item \textbf{Hate/offensive:-} Every half-breed cop in America is trying to rape white women.
%         \item \textbf{Implication:-} People of color are rapists (These can be close to one of the topics in the debate)
%     \end{itemize}
%     \item Each side of an argument in a debate is assigned a pro or anti stance. In the form of raising a dispute, there are often direct responses to a specific claim.
%     \item These direct responses can be combined to form a pair, which will aid the model in determining how to respond to a specific statement. Pre-training on such a subject will benefit the generation because these comments are similar to the real hate topics. The complete data-extraction pipeline can be summarized in the Fig -\ref{fig:DATA-CREATE}.
% \end{itemize}


% The final collected support and disputed-instance pairs have the following statistics:
% \begin{table}[h]
%  \centering
% \begin{tabular}{|c|c|c|}
%  \hline
%  \textbf{Type of Pairs} & \textbf{Average per discussion} & \textbf{Total data(rows)} \\
%  \hline
%   \textbf{Disputed}   &  41 &  21657 \\
%  \hline
%   \textbf{Clarified} & 7 & 3755 \\
%  \hline 
%   \textbf{Supported} & 7 & 4190\\
%   \hline
% \end{tabular}
% \caption{Data-Statistics used in the experiment}
% \label{Data_Stats}
% \end{table}

% \newpage

% \section{Proposed Model}

% We finally describe our system architecture for counter-speech generation. We used Dialo-GPT(\textbf{\cite{zhang2019dialogpt}}) as the language model for generating counter-arguments to the given initiator argument. The architecture for the proposed model is given below in the \textbf{Fig -\ref{fig:sysa}}.

% \begin{figure}[h]
%     \centering
%     \includegraphics[scale=1.0,width=1.0\linewidth]{Graphics/System(a).jpg}
%     \caption{System-Architecture for Zero-Shot Learning}
%     \label{fig:sysa}
% \end{figure}

% We will try to evaluate the generation quality in this zero-shot fashion by feeding the supported-disputed instances to the model and generating the output counter-arguments on hate-instances from REDDIT, GAB and CONAN.

% \section{Challenges Involved with current model : }
% \begin{itemize}
%     \item We might not be able to map the text of hate speech to the discussion subject because the language of hate speech is so toxic. Again, the implications will serve as a bridge.
%     \item When providing the input to Dialo-GPT, the length of the text is a problem. To make the text shorter, we planned to use summarization technique. For the experiments, we truncated the initiator and counter arguments to \textbf{256 tokens}.
%     \item We can also get toxic replies because they aren't exactly moderated, so we're going to use a few detoxifying and politeness discriminators that we can do with PPLM.
    
% \end{itemize}

% \section{Proposed Model with De-toxifiers}

% \begin{figure}[h]
%     \centering
%     \includegraphics[scale=1.0,width=1.0\linewidth]{Graphics/System(b).jpg}
%     \caption{System-Architecture with De-Toxifiers}
%     \label{fig:sysb}
% \end{figure}

% The proposed architecture with modifications is described in the \textbf{Fig-\ref{fig:sysb}}. The advantages of this proposed model can be listed as below:
% \begin{itemize}
%     \item Considering the time and effort required to generate responses, it is inexpensive (cost of annotation), but it may be less reliable (CONAN). A moderator will make minor changes to these responses before publishing them.
%     \item  More diverse than the Reddit and Gab datasets, which generate template examples.
% \end{itemize}

