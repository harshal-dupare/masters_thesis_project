\newpage
\level{0}{Conclusion and Future Work}

 \paragraph{} In conclusion, we have presented a comprehensive study of solving cond-fMOOP with restrictions on condition number, which generalizes several important problems. We have reviewed various methods to solve MOOPs and discussed important results on the condition number of matrices and functions. We also observed that bounding the condition number of neural networks is a viable defensive mechanism against adversarial attacks. We have formulated some critical questions about cond-fMOOP and derived two theoretical results on the condition number of composition of functions and the relationship between $L_2$ regularization and absolute condition number. We have then focused on the original problem which motivated the definition of fMOOP i.e. the problem of Linear Factor Model with the condition $AA^\top = I$ and proposed a method that leverages the property $AA^\top=I$ to keep $A$ satisfying this condition while updating it based on a designed update scheme. Our experiments have shown that this property can help the methods to converge smoothly towards a solution.
 
 It would be interesting to conduct further studies focusing on determining what other problems fall into cond-fMOOP category. Developing a class of methods to address this problem will be a significant step forward. Furthermore, addressing any of the three critical questions that we have mentioned can further improve the space of methods for solving the general problem and the problems contained in it. Additionally, future works can also explore other properties formulations of the loss and reward functions to optimize the computation of the solution. Finally, exploring other potential applications of the methods presented in this project beyond the Linear Factor Model problem is another avenue for future research.




